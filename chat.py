from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
import streamlit as st

from dotenv import load_dotenv

load_dotenv()


# Define a sessÃ£o do Streamlit
st.set_page_config(page_title="Jasmine - Assistente de Documentos", layout="wide")
st.title("ğŸ“„ Jasmine - Despachante Inteligente")
st.markdown("FaÃ§a uma pergunta com base no conteÃºdo do PDF. Jasmine te responde na hora!")


caminho_pdf = "servico.pdf"
# Lendo arquivo pdf
loader = PyPDFLoader(caminho_pdf)
lista_docs = loader.load()
documentos_pages = [page for page in lista_docs]

# Carrega e indexa o PDF apenas uma vez por sessÃ£o
@st.cache_resource(show_spinner="Carregando e indexando o PDF...")
def carregar_vectorstore():    

    # splitando os documentos 
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50, length_function=len, separators=["\n\n"], is_separator_regex=False)
    documents_splits =  splitter.split_documents(documentos_pages)

    # indexando vectorstore e fazendo o embedding
    vectorstore = FAISS.from_documents(documents_splits, OpenAIEmbeddings())
    return vectorstore


vectorstore = carregar_vectorstore()
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})

# DefiniÃ§Ã£o do template
template = """
VocÃª Ã© Jasmine, uma despachante experiente, clara e eficiente. Sempre inicie sua resposta se identificando como Jasmine.

Responda Ã  pergunta do usuÃ¡rio com base **exclusivamente** nas informaÃ§Ãµes fornecidas no contexto abaixo.

ğŸ“Œ InstruÃ§Ãµes importantes:
- Todas as informaÃ§Ãµes como telefone, endereÃ§o ou nome presentes no documento devem ser consideradas como pertencentes Ã  empresa **Grupo Andrade**, salvo indicaÃ§Ã£o explÃ­cita em contrÃ¡rio.
- Caso a resposta **nÃ£o possa ser respondida com o contexto fornecido**, diga isso de forma educada, e em seguida **sugira uma versÃ£o melhorada da pergunta**, para que o usuÃ¡rio possa copiÃ¡-la e colÃ¡-la novamente.

---

ğŸ“„ Contexto:
{context}

â“ Pergunta:
{question}

---

ğŸ‘©â€ğŸ’¼ Resposta (por Jasmine, com tom humano e profissional):
"""

prompt = PromptTemplate(template=template, input_variables=["context", "question"])
llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
parser = StrOutputParser()

chain = prompt | llm | parser

with st.form("form_pergunta"):
    pergunta = st.text_input("Digite sua pergunta sobre o documento:", placeholder="Ex: Qual o paÃ­s do endereÃ§o desse recibo?")
    enviar = st.form_submit_button("Perguntar", disabled=False)

if enviar and pergunta:
    contextos = retriever.invoke(pergunta)
    contexto = "\n".join((f"Source: {ctx.metadata}\n\n" f"Content: {ctx.page_content}") for ctx in contextos)
    print(contexto)
    st.markdown("### âœ¨ Resposta da Jasmine:")
    resposta_area = st.empty()
    # Exibir resposta com streaming
    resposta_final = ""
    try:
        for chunk in chain.stream({"context": contexto.lower(), "question": pergunta}):
            resposta_final += chunk
            #print(resposta_final)
            resposta_area.markdown(f"ğŸªª {resposta_final}â–Œ")
        resposta_area.markdown(f"ğŸªª {resposta_final}")  # Remove o cursor final

    except Exception as e:
        st.error(f"Erro ao gerar resposta: {e}")
